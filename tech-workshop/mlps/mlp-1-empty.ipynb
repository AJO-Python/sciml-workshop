{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#plt.style.available\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron\n",
    "\n",
    "In this practical we fit our first deep neural network. We are starting with the simplest architecture - a multi-layer perceptron (MLP). \n",
    "\n",
    "## The task\n",
    "\n",
    "We have a set of spectra from Muon spectroscopy experiemnts, we would like to train a NN to be able to detect from a given spectrum wheter or not a certain element is present in the sample. In this case we are going to try to train a network to detect if there is `Ag` in a sample.\n",
    "\n",
    "## The data\n",
    "\n",
    "There is a dataset conveniently provided with spectra and labels saying wheter or not there is `Ag` in the data (`ag-muon-data.pkl`). It is in a `pickle` file, load it into a `pandas` dataframe and take a quick look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('ag-muon-data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the third column of the dataframe contains either 1 or zero, conveniently the data has already been one-hot-encoded for you. 1 - there is `Ag` in the sample, 0 - there is no `Ag` in the sample. The fifth column of the dataframe contains the spectroscopic signal from the experiment. \n",
    "\n",
    "The sample number `23993` is pure `Ag` plot it out to see the kind of data we are looking at.\n",
    "\n",
    "*Note* To access this data in `pandas` use `sample = df.iloc[23993][3]`\n",
    "\n",
    "Use `plt.plot(sample)` to look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some clear peaks associated with `Ag` - however, to get a feel for the complexity of picking out signals with `Ag` in multinary samples try plotting sample number `26`, which is an `Ag/Si` binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The approach\n",
    "\n",
    "We are going to try and train a NN to classify spectra into those that contain `Ag` and those that do not.\n",
    "\n",
    "Get `X` - your input data as the fifth column (labelled `3` in the dataframe) of the dataset and `y` your objective as the third column (labelled `1` in the dataframe). You can use the pandas function `to_list` to do this. Convert the resulting lists to `numpy` arrays for feeding to the NN.\n",
    "\n",
    "```\n",
    "X = np.array(df[3].to_list())\n",
    "y = np.array(df[1].to_list())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out a network\n",
    "\n",
    "We can see that the data is given as a series of 300 points, each corresponding to an intensity. We can think about building a MLP that has 300 nodes in the input layer and then a single node in the output layer. The output should be 1 if there is `Ag` in the sample and 0 if there is no `Ag` in the sample.\n",
    "\n",
    "We will start by using `keras` `Sequential` model building - this allows you to simply add new layers to a model using the `model.add(<layer details>)` command. The model is instantiated with\n",
    "```\n",
    "model = Sequential()\n",
    "```\n",
    "Now we want to add layers to the netowrk - as we are building a MLP, all of our layers are fully connected or [`Dense`](https://keras.io/layers/core/) layers. The arguments that are passed to the dense layer are\n",
    "* Number of nodes\n",
    "* Number of inputs to the layer - this is only necessary for the first layer and must match the shape of the data\n",
    "* Activation function - this is the non-linear function used to calculate the activation of the layer \n",
    "* There are many other properties you can set, but these are all we need for now\n",
    "\n",
    "**Note** we are importing from `tensorflow.keras`, rather than `keras` this is to ensure consistency with models built in pure `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=300, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at the properties of the model with `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to [compile](https://keras.io/models/model/) the model - this involves setting a `loss` to train against, an `optimizer` and optionally some `metrics` to evaluate during the training. For the `loss`, since we are fitting to a range between 0 and 1 `binary_crossentropy` is the obvious choice. For `optimizer` we use `adam` and we will have just one metric `accuracy` - note that metrics must be passed as a list. See the [`keras` documentation](https://keras.io/models/model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is compied we can now start to fit it using the data. Again details of the `fit` function of `keras` models can be found in the [`keras` documentation](https://keras.io/models/model/). \n",
    "\n",
    "We need to specify the X and y data, we will run for 60 epochs and we will use a batch size of 32. We should also specify a validation split of 20% of the data using `validation_split=0.2` Run the model for `20` epochs with a `batch_size` of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "\n",
    "Notice how the accuracy of the model very quickly achieves a certain value and then stops improving. This value could be well above 50%, which suggests that that model is actually learning something. However we may have fallen into a trap here. Take a look at the distribution of the target data, using `plt.hist(y)`. Also pay special attention to the validation data (the final 20% of the data `y[21000:]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice something about the class numbers that closely resembls the accuracy achieved by the model. The data with no `Ag` accounts for around 95% of the data. So if the model simply learns to guess no `Ag` it can achieve 95% accuracy without learning anything useful. To avoid this we will rebalabace the classes.\n",
    "\n",
    "With class imbalance there are a number  of strategies that you can take to address the problem.\n",
    "\n",
    "* Up-sample the minority class\n",
    "* Down-sample the majority class\n",
    "* Change your performance metric\n",
    "\n",
    "In our case the best available option is to down-sample the majority class, so that `Ag` and no`Ag` have equal numbers in the datasets.\n",
    "\n",
    "You can split the dataframe into the `1` and `0` entries using something like `df[df[1]==0]`.\n",
    "\n",
    "Then use [`sklearn.utils.resample`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html) to resample the majority dataframe.\n",
    "\n",
    "Then concatenate and shuffle the datasets - `pd.concat` followed by `pd.sample`\n",
    "\n",
    "A good online guide is available [here](https://elitedatascience.com/imbalanced-classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "num_ag = np.count_nonzero(y==0)\n",
    "num_no_ag = np.count_nonzero(y==1)\n",
    "\n",
    "df_majority = df[df[1]==0]\n",
    "df_minority = df[df[1]==1]\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=num_no_ag,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "df_new = df_downsampled.sample(frac=1)\n",
    "\n",
    "X = np.array(df_new[3].to_list())\n",
    "y = np.array(df_new[1].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the network \n",
    "\n",
    "Rerun the `model.fit()` command from above using the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** In some instances the model now stops learning at around 50% - pretty disappointing. So what else could be holding us back? In some instances the model will actually train and achieve above 50% in this step - the explanation is below.\n",
    "\n",
    "## Optimizer hyperparameters\n",
    "\n",
    "Recall from the lecture that we talked about hyperparameters in optimizers. In this case we have used the `Adam` optimizer with a default learning rate. \n",
    "\n",
    "#### \"The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate.\" - Goodfellow, Deep Learning\n",
    "\n",
    "One thing that can happen if our learning rate is too high initially is that the optimizer can continually jump over optimal solutions and not actually find its way to a good answer. Whether or not the optimizer does find it's way to a minmum is down to a combination of the loss function space, the starting parameters and the optimizer settings. Since the initial parameters of the network are randomised there are some cases, as noted above where, the network will optimise at the standard learning rate. However, sometimes it will just keep skipping around and not learning. \n",
    "\n",
    "To overcome this problem we can lower the learning rate of the optimizer for an initial optimisation. Once the model has started to train and the loss has started to decrease we can dial the learning rate back up to speed up the training.\n",
    "\n",
    "On the other hand, lower the learning rate too much and training will take an eternity.\n",
    "\n",
    "### Grid search for learning rate\n",
    "\n",
    "For this part of the exercise we will do a serach over possible learning rates. Typically we would look over  range of 0.1 to 10$^{-5}$ on a $\\log$ scale.\n",
    "\n",
    "To edit the optimizer settings, we import the optimizer `Adam` from `tensorflow.keras.optimizers` and set the learning rate\n",
    "\n",
    "```\n",
    "ad = Adam(learning_rate=0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "Construct a loop to go over the learning rate range described above. For each training cycle we will extract the best validation accuracy value.\n",
    "\n",
    "You can save the history of a training cycle as a dictionary simply by `history = model.fit(<params>)`, here the metrics and losses are saved to a dictionary `history.history`. Run the model for 40 epochs at each learning rate.\n",
    "\n",
    "Save this dictionary for each learning rate and then compare the results. Look at maximum value for `accuracy` from each of the learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "lr_histories = []\n",
    "for lr in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=300, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    ad = Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "    history = model.fit(X, y, epochs=40, batch_size=64, validation_split=0.2)\n",
    "    lr_histories.append(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = [max(h['accuracy']) for h in lr_histories]\n",
    "plt.plot(maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train the model at the identified learning rate for 100 epochs\n",
    "\n",
    "Then plot the training history of this run - again taken from the `history.history` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=300, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=100, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "print(max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed up the learning\n",
    "\n",
    "Even when we are using a slower learning rate, it is possible to speed up the training process. `BatchNormalization` is covered in the lectures and involves normalising all values going into a layer before the forward pass. Although it introduces some small extra computational cost into the forward pass, the results for speeding up learning rate can be spectacular - particularly in the deeper networs that we will look at in the next part of this exercise.\n",
    "\n",
    "For now, import `BatchNormalization` from `tensorflow.keras.layers` and add a `BatchNormalization()` unit between each of your `Dense` layers.\n",
    "\n",
    "Retrain the model and look at the learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=300, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "history_bn = model.fit(X, y, epochs=100, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "Plot the same stats as for the previous training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some other metrics\n",
    "\n",
    "Lets look at some other metrics of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y\n",
    "y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "y_pred = np.rint(y_pred)\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, X, y,\n",
    "                        display_labels=['No Ag', 'Ag'],\n",
    "                        cmap=plt.cm.Blues,\n",
    "                                 normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
