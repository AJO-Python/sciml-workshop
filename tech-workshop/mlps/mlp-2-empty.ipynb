{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#plt.style.available\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger data - deeper networks\n",
    "\n",
    "The last exercise gave us an intro into how to build and tune some of the parameters of a neural network. Now we move to an example where we have more data. Generally as we increase the amount of training data we can make use of deeper networks, with more layers to give more accurate predictions.\n",
    "\n",
    "This time around we load up the data from `ag-muon-data-tight.pkl`\n",
    "\n",
    "You can load this up in the same way as in the previous notebook. We just take the first 90k examples to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('ag-muon-data-tight.pkl').iloc[:90000]\n",
    "X = np.array(df[3].to_list())\n",
    "y = np.array(df[1].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance \n",
    "Don't forget to take care of class balance in your dataset. Do the same checks and use the `resample` function as you did in the previous notebook. Plot a historgram of the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial network\n",
    "\n",
    "We now have more data, so we could think about making a deeper network. Try out the architecture below. **Note** this needs to run for more epochs as the network takes some time to equilibrate, so a little bit of patience is needed. If you like you can try stopping it earlier (running fewer epochs), then plot the training and validation loss to see how it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=1000, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "ad = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "history_bn = model.fit(X, y, epochs=200, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot your training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
