{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a VAE (+NN+MNIST)\n",
    "\n",
    "This is a an implementation of the \"Auto Encoding Variational Bayes\" paper [here](https://arxiv.org/abs/1312.6114).\n",
    "\n",
    "The idea is to reconstruct the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) using VAE. A simple neural network is used as encoder and decoder than anything complicated.  \n",
    "\n",
    "## What are VAEs\n",
    "\n",
    "Autoencoders are a type of neural network that can be used to learn efficient codings of input data. Given some inputs, the network first applies a series of transformations that map the input data into a lower dimensional space. This part of the network is called the ***encoder***, and implements the posterior $q_\\theta(z|x)$.\n",
    "\n",
    "Then, the network uses the encoded data to try and recreate the inputs. This part of the network is the ***decoder***, denoted by another pdf $q_\\phi(x | z)$. Using the encoder, one can compress the data of the type that is understood by the network. However, autoencoders are rarely used for this purpose, as usually there exist hand-crafted algorithms (like jpg-compression) that are more efficient.\n",
    "\n",
    "VAE is an extention of AEs, with applications including, \n",
    "- in painting\n",
    "- denoising \n",
    "- generating artificial faces, scenes etc. \n",
    "\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "### Derivation of ELBO\n",
    "\n",
    "Posterior Probability $p(z|x)$, which can be expressed as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "p(z|x)&=&\\frac{p(x|z)p(z)}{p(x)}\\nonumber\\\\\n",
    "&=& \\frac{p(x|z)p(z)}{\\int p(x|z)p(z)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $\\int p(x|z)p(z)$, which is the marginal, can be intractable and cannot be computed directly. One way  to compute the overall solution $p(z|x)$ is using Monte Carlo methods (such as sampling). The method used in this notebook (and the underlying VAE paper) is  ***variational inference***. \n",
    "\n",
    "The idea is to identify another proxy distribution $q(z|x)$ that reasonably approximates $p(z|x)=p(x|z)p(z)$. i.e. if the KL-divergence between two pdfs, $q(x)$ and $p(z|x)$ is denoted by\n",
    "\n",
    "$$\\mathrm{KL}(q(x)||p(z|x))$$\n",
    "\n",
    "\n",
    "it can be minimized by selecting an alternative pdf $q(z|x)$, which is a good proxy for $p(z|x)$. But \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{KL}(q(z|x)||p(z|x)) &=& -\\int q(z|x)\\log\\frac{p(z|x)}{q(z|x)} dz\\nonumber\\\\\n",
    "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{p(x)q(z|x)} dz\\nonumber\\\\\n",
    "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)}dz + \\int_{z} q(z|x)\\log p(x)dz \\nonumber\\\\\n",
    "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)} +  \\log p(x)\\int_{z} q(z|x)dz\\nonumber\\\\   \n",
    "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)}dz +  \\log p(x)\\nonumber\\\\\n",
    "                          &=& -\\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz -\\int q(z|x)\\log{p(x|z)}dz + \\log p(x) \n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "Given that $\\mathrm{KL}\\left(q(z|x)||p(z|x)\\right)\\geq 0$, \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "-\\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz -\\int q(z|x)\\log{p(x|z)}dz + \\log p(x) &\\geq& 0 \\\\\n",
    "\\log p(x) &\\geq& \\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz + \\int q(z|x)\\log{p(x|z)}dz\\\\\n",
    "\\log p(x) &\\geq& - \\mathrm{KL}(q(z|x)||p(z)) + \\int q(z|x)\\log p(x|z)dz \\nonumber\\\\\n",
    "\\log p(x) &\\geq& - \\mathrm{KL}(q(z|x)||p(z)) + \\mathbb{E}_{q(z|x)}\\left[\\log p(x|z)\\right] \\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "This is the *variational lower-bound*, or the evidence of lower bound (ELBO).  This remains as the objective function for the VAE. However, frameworks like TensorFlow or PyTorch need a loss function to be minimized. Maximising the log likelihood of the model evidence $p(x)$ is same as minimizing the $-\\log p(x)$. The first term of the ELBO, namely,  $\\mathrm{KL}(q(z|x)||p(z))$ is the *regularising* term and constrains the posterior distribution. The second term of the ELBO models the reconstruction loss. \n",
    "\n",
    "Now, this leaves fair bit of freedom on the choice of the prior $p(z)$. Let's assume:\n",
    "\n",
    "\n",
    "$$\n",
    "p(z)={\\cal N}(\\mu_p, \\sigma_p^2)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "q(z|x)={\\cal N}(\\mu_q, \\sigma_q^2)\n",
    "$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "p(z)=\\frac{1}{\\sqrt{2\\pi\\sigma_p^2}}\\exp\\left(\\frac{(x-\\mu_p)^2}{2\\sigma_p^2}\\right)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "q(z|x)=\\frac{1}{\\sqrt{2\\pi\\sigma_q^2}}\\exp\\left(\\frac{(x-\\mu_q)^2}{2\\sigma_q^2}\\right)\n",
    "$$\n",
    "\n",
    "The direct derivation of $\\mathrm{KL}(q(z|x)||p(z))$ will give (with some simplifications)\n",
    "\n",
    "\n",
    "$$\n",
    "-\\mathrm{KL}(q(z|x)||p(z)) = \\log\\frac{\\sigma_q}{\\sigma_p} - \\frac{\\left(\\log\\sigma_q^2-(\\mu_p-\\mu_q)^2\\right)}{2\\sigma_p^2} +\\frac{1}{2} \n",
    "$$\n",
    "\n",
    "By fixing the prior distribution $p(z)={\\cal N}(0,1^2)$, \n",
    "\n",
    "$$\n",
    "-\\mathrm{KL}(q(z|x)||p(z)) = \\frac{1}{2}\\left[ 1 + \\log\\sigma_q^2 - \\sigma_q^2 -\\mu_q^2\\right]\n",
    "$$\n",
    "\n",
    "Hence, the new ELBO is\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\left[ 1 + \\log\\sigma_q^2 - \\sigma_q^2 -\\mu_q^2\\right] + \\mathbb{E}_{q(z|x)}\\left[\\log p(x|z)\\right] \n",
    "$$\n",
    "\n",
    "\n",
    "Let $J, B$ and $\\cal{L}$ be the dimension of the latent space, and the batch size over which the sampling is done. The loss function we need to minimise (from the point of  implementation) is\n",
    "\n",
    "$$\n",
    "{\\cal L} = - \\sum_{j=1}^J \\frac{1}{2}\\Bigl[ 1 + \\log\\sigma_j^2 - \\sigma_j^2 -\\mu_j^2\\Bigr] - \\frac{1}{B}\\sum_{l}\\mathbb{E}_{q(z|x_i)}\\left[\\log p(x_i|z^{(i,l)})\\right] \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "This can be observed in the code implementation below (see function implementation ``loss_function`` below)\n",
    "\n",
    "### Reparameterisation\n",
    "\n",
    "A valid reparameterization would be \n",
    "\n",
    "$$\n",
    "z = \\mu+\\sigma\\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\epsilon$ is an auxiliary noise variable $\\epsilon\\sim{\\cal{N}}(0, 1)$, which actually enables the reparameterization technique. Although it is possible to use $\\sigma$ or more specifically $\\sigma^2$, working on log scales improves the stability. i.e. \n",
    "\n",
    "\\begin{eqnarray}\n",
    "p &=& \\log(\\sigma^2)\\\\\n",
    "&=& 2 * \\log(\\sigma)\n",
    "\\end{eqnarray}\n",
    "\n",
    "To get the log standard deviation, $\\log(\\sigma)$, \n",
    "\\begin{eqnarray}\n",
    "\\log(\\sigma) &=& 0.5 p \\\\\n",
    "\\label{eqn:log_sigma}\n",
    "\\end{eqnarray} \n",
    "\n",
    "and hence\n",
    "\n",
    "$$\n",
    "\\sigma = \\exp^{0.5p}\n",
    "$$\n",
    "\n",
    "The resulting estimator (or the loss function) becomes (see Page 5 of [Auto-Encoding Variational Bayes Paper](https://arxiv.org/abs/1312.6114)),\n",
    "\n",
    "$$\n",
    "-KLD = \\frac{1}{2}\\sum_{j=1}^{J}(1+\\log(\\sigma_j^i)^2 - (\\mu_j^i)^2 -(\\sigma_j^i)^2)\n",
    "$$\n",
    "\n",
    "\n",
    "It is important to see that the KL divergence can be computed and differentiated without estimation. This is a very remarkable thing (no esimtation!).\n",
    "\n",
    "The $\\boldsymbol{\\epsilon}$ must be sampled from a zero-mean, unit-variance Gaussian distribution, and should be of the same size as $\\boldsymbol{\\sigma}$. This is easily done using PyTorch's **``randn_like``** function. The PyTorch document states that \n",
    "\n",
    "> ``torch.randn_like(input)`` is equivalent to ``torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Variational Bayes [Paper](https://arxiv.org/pdf/1312.6114.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Implementation \n",
    "\n",
    "The implementation below has the following:\n",
    "\n",
    "1. A loss function from the paper\n",
    "1. A class implementation for VAE\n",
    "1. An instance of the VAE\n",
    "1. Training of the VAE\n",
    "1. Testing of the VAE\n",
    "1. Visualization of the latent space (uses tSNE)\n",
    "1. Visualization of the posteriors\n",
    "1. Implementation of two types of generative process to show the limitations of vanilla VAE.\n",
    "\n",
    "\n",
    "Again, the implementation here is not optimized nor optimal. For example, there are repeated computations on the latent space, which can easily be avoided by caching some results from training etc. \n",
    "<BR>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current device for execution using PyTorch's 'device' method\n",
    "device=torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters of the models\n",
    "# Set some initial values. Changing batch size and epochs has a knock \n",
    "# on effect on the overall performance. \n",
    "BATCH_SIZE  = 128\n",
    "EPOCHS=10\n",
    "VERBOSE = True\n",
    "LEARNING_RATE = 1e-3\n",
    "USE_MULTIPLE_GPUS = False\n",
    "\n",
    "# network architecture\n",
    "# size of latent space\n",
    "Z_DIM = 20\n",
    "# size of hidden layer in encoder and decoder\n",
    "HIDDEN_DIM = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tic():\n",
    "    t0 = timeit.default_timer()\n",
    "    return t0\n",
    "\n",
    "def toc(t0):\n",
    "    t1 = timeit.default_timer() - t0\n",
    "    return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the description above. \n",
    "# The loss function has two parts: KLD, and the conventional loss function (such as binary cross entropy).\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = torch.nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The VAE Class\n",
    "\n",
    "This class inherits the **``nn.Module``** from **``torch.nn``**. As such, this has to implement the **``forward``** component (and the backward is automatically obtained by autodiff inside **``nn.Module``**.  The class also implements the following:\n",
    "\n",
    "1. Encoder\n",
    "1. Decoder; and\n",
    "1. Reparameterisation.\n",
    "\n",
    "### Encoder and Decoder\n",
    "\n",
    "The exact architecture is independent of the Variational Bayes paper, and may vary with the problem.  Here, the architecture is such that:\n",
    "\n",
    "- An encoder based on NN with Gaussian outputs - vector of means, and vector of standard deviations (or variances)\n",
    "- Means are constrained between $[0,1]$ using sigmoid\n",
    "- The MNIST has $28\\times 28$ images with one colour channel - so the encoder/decoder inputs/outputs reflect this.\n",
    "\n",
    "\n",
    "However, the encoder can include a CNN instead of a simple neural net, and can use regularisers, dropout etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, HIDDEN_DIM)\n",
    "        self.fc21 = nn.Linear(HIDDEN_DIM, Z_DIM)\n",
    "        self.fc22 = nn.Linear(HIDDEN_DIM, Z_DIM)\n",
    "        self.fc3 = nn.Linear(Z_DIM, HIDDEN_DIM)\n",
    "        self.fc4 = nn.Linear(HIDDEN_DIM, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and execution - PyTorch specifics\n",
    "\n",
    "It is worth noting the following on the **training** end \n",
    "\n",
    "\n",
    "* In PyTorch, calling ``model.train()`` or ``model.eval()``  sets the mode of execution. Possible modes of execution are:  training and evaluation. However, they **do not** inovoke the training or evaluation. \n",
    "* Calling the ``model(data)`` method invokes the forward pass on the VAE.\n",
    "* Calling the ``optimizer.zero_grad()`` method resets all gradients to zero.\n",
    "* A call to the ``loss.backward()`` method performs the back propagation (which uses autograd).\n",
    "* A call to the ``data.to(device)`` method moves the data (which we got from the train_loader) to GPU.\n",
    "* <font color='red'>There is a bug when running this across multiple GPUs. Need to debug this. At least it is slower (perhaps MNIST is not big enough). I  haven't done much profiling here. </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_train_stats(batch_idx, epoch, data, train_loader, loss):\n",
    "    if VERBOSE == True and batch_idx % 10 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), \n",
    "                                                                       100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "\n",
    "def print_train_epoch_stats(epoch, train_loss):\n",
    "    if VERBOSE == True:\n",
    "        print('====> Epoch: {} Average Training loss: {:.4f}'.format(epoch, train_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        print_train_stats(batch_idx, epoch, data, train_loader, loss)\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader.dataset)    \n",
    "    print_train_epoch_stats(epoch, train_loss)\n",
    "    return train_loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing \n",
    "\n",
    "Note the ``torch.no_grad()`` - which turns off the back propagation (which is not needed in the validation phase). Disabling gradient calculation is useful for inference when it is certain that Tensor.backward() will not be called. It will reduce memory consumption for computations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_epoch_stats(epoch, test_loss):\n",
    "    if VERBOSE == True:\n",
    "        print('====> Epoch: {} Test set loss: {:.4f}'.format(epoch, test_loss))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print_test_epoch_stats(epoch, test_loss)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the datasets \n",
    "\n",
    "Although datasets can be loaded in their own, ideally, we may want to specify the transformations to be performed on those datasets while loading them.  See [this](https://pytorch.org/docs/stable/torchvision/datasets.html) link for more details. The actual data loading is performed by the **``DataLoader``** class. The data loader has many capabilities, including chunking/partitioning the data as needed. For example, DataLoader  can load multiple samples in parallel using torch.multiprocessing workers.\n",
    "\n",
    "The following are the key parameters \n",
    "\n",
    "1. root (string) – Root directory of dataset where training and testings dataset exists. \n",
    "1. train (bool, optional) – If True, creates dataset from training side  otherwise from test side. \n",
    "1. download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n",
    "1. transform (callable, optional) – A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop\n",
    "1. target_transform (callable, optional) – A function/transform that takes in the target and transforms it.\n",
    "\n",
    "\n",
    "<BR> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hold of the MNIST dataset\n",
    "# Also note that the ToTensor call in PyTorch automatically converts all images into [0,1]. \n",
    "train_dataset = datasets.MNIST(root='./data/mnist/', train=True, transform=transforms.ToTensor(),  download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/mnist/', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an instance of the VAE class\n",
    "\n",
    "Create the model (which is a VAE here), and move that to the GPU. This is needed before constructing the optimizer object. Check the Pytorch manual [here](https://pytorch.org/docs/stable/optim.html). Then check the learning rate to $\\lambda = 10^{-3}$. \n",
    "    \n",
    "The model is created and moved to the preferred device. The data is moved to the devide during the training  / testing phase. The testing phase, does not use optimiser or the back-propagation.     \n",
    "\n",
    "### Running Train / Test  per  epoch\n",
    "\n",
    "It is worth noting that epoch is a one full cycle through the training data. Usually, the model is validated after each training epoch to get a signal about the model’s ability to generalize, i.e. how high the validation accuracy is (or how low the validation loss gets). Using these validation techniques like early stopping can be applied in order to stop the training once  the model starts to overfit. Running the validation (or test) for several epochs sequentially doesn’t make sense, as the metrics won’t change. \n",
    "    \n",
    "Inside the epoch loop, decoding on a random sample is performed just to see a visualization of the generated image from a random sample. Please note, the test(epoch) function gives the quantitative value which tells  how well the model performs. However, the image generation is intended for qualitative comparison, i.e., how the model improves image generation with each epoch. This can be skipped without any issues. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "if USE_MULTIPLE_GPUS == True and torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Using DataParallel Model\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses, test_losses = [], []\n",
    "t0=tic()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "tlapsed = toc(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss');\n",
    "plt.plot(test_losses, label='Validation loss');\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.title('Training/Validation Loss with Epochs')\n",
    "plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the hidden states\n",
    "\n",
    "We only visualize the mu because mu represents corresponding latent vectors to inputs while variance is a degree of diversity of each latent vector. So latent representation of variance is not of interest to us. We will use the TSNE (sadly very slow version from scikit) for this purpose, along with matplotlib. Note that we terminate the plot lines with semicolon to avoid junk messages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the training data without gradient - this may not be necessary\n",
    "# The original training data can be saved for this purpose. Oh well.\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Note that the primary dimension is set to 0. \n",
    "h_mu = torch.zeros((0,Z_DIM), device=device)\n",
    "h_logvar = torch.ones((0,Z_DIM), device=device)\n",
    "h_labels = torch.ones(0, dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        _, x_mu, x_logvar = model(data)\n",
    "        h_mu = torch.cat( (h_mu, x_mu), 0)\n",
    "        h_logvar = torch.cat((h_logvar, x_logvar), 0)\n",
    "        h_labels = torch.cat( (h_labels, target))\n",
    "\n",
    "h_mu = h_mu.cpu()\n",
    "h_logvar = h_logvar.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting t-SNE is extremely expensive\n",
    "# be careful about using a large PLOT_TSNE_SIZE\n",
    "PLOT_TSNE = True\n",
    "PLOT_TSNE_SIZE =1000\n",
    "if PLOT_TSNE == True:\n",
    "    tsne_mnist = TSNE(learning_rate=200, perplexity=40, \n",
    "                      n_iter=5500).fit_transform(h_mu.detach().numpy()[:PLOT_TSNE_SIZE])\n",
    "    plt.scatter(tsne_mnist[:PLOT_TSNE_SIZE, 0], tsne_mnist[:PLOT_TSNE_SIZE, 1],  \n",
    "                c=h_labels[:PLOT_TSNE_SIZE],  cmap = plt.cm.get_cmap(\"jet\", 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Distributions \n",
    "\n",
    "Now, let's look at the individual conditional distributions, $p(z|y)$, where $y$ is the label, and $z$ is the latent variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for getting custom text\n",
    "def getCustomTitle(pdf_x, digit_index):\n",
    "    mean = \"{:.2f}\".format(torch.mean(pdf_x).item())\n",
    "    var = \"{:.2f}\".format(torch.var(pdf_x).item())\n",
    "    title_str = 'Distribution for Digit %i' %digit_index + '\\n' \n",
    "    title_str += r'${\\cal N}(\\mu=$' + mean + r'$,\\sigma=$' + var + r'$)$'\n",
    "    return title_str "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own custom 2-in-a-row dist plotter\n",
    "def customDistPlot(axes, pdf_a, pdf_b, i):\n",
    "    mean_a, var_a, mean_b, var_b = torch.mean(pdf_a).item(), torch.var(pdf_a).item(), torch.mean(pdf_b).item(), torch.var(pdf_b).item()\n",
    "\n",
    "    sns.distplot(pdf_a, color='g', ax=axes[0]);\n",
    "    sns.distplot(pdf_b, color='b', ax=axes[1]);\n",
    "    \n",
    "    axes[0].set_title(getCustomTitle(pdf_a, i));\n",
    "    axes[1].set_title(getCustomTitle(pdf_b, i+1));\n",
    "    \n",
    "    axes[0].axvline(mean_a, color='red');\n",
    "    axes[1].axvline(mean_b, color='red');\n",
    "        \n",
    "    axes[0].grid(True);\n",
    "    axes[1].grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SEPRATE_PDF_PLOTS = True\n",
    "if SEPRATE_PDF_PLOTS == True:\n",
    "    for i in range(0,10,2):\n",
    "        f, axes = plt.subplots(1,2,sharex=False, sharey='row');    \n",
    "        pdf_a, pdf_b = h_mu[h_labels==i,:], h_mu[h_labels==i+1,:]\n",
    "        customDistPlot(axes, pdf_a, pdf_b, i);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show all distributions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_PDF_PLOT = True\n",
    "if COMBINED_PDF_PLOT == True:\n",
    "    plt.figure()\n",
    "    for i in range(0,10):\n",
    "        pdf_a = h_mu[h_labels==i,:]\n",
    "        sns.distplot(pdf_a)\n",
    "    plt.grid(True)\n",
    "    plt.title(r'Distribution of $p(z|x=\\{0,1,\\ldots,9\\})$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that all posteriors, $p(z|x=\\{0,1,\\ldots,9\\})$, overlap and there is almost no separation. This is likely to challenge the sampling process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "Now, we got the posterior $p(z|x)$ in place. We can try and generate some digits from these now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sampling from the Normal distribution\n",
    "\n",
    "All posteriors, are overlapping and they all appear to be a normal distribution with $\\mu=0$, and $\\sigma=1$. We can exploit this for the data generation. To do this, we need an internal representation $z$, matching the dimensions we specified above, $20 x 20$. Once we have the $z$, we can pass this to the decoder (without backprop), to generate $\\hat{x}$. i.e. we create a latent representation $z$ by sampling a normal distribution $N(\\mu=0,\\sigma^2=1)$. Consult the  ``torch.distributions.Normal``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.distributions.Normal(torch.zeros(1,Z_DIM), torch.ones(1,Z_DIM)).sample().to(device)\n",
    "with torch.no_grad():\n",
    "    x_hat = model.decode(z)\n",
    "x_hat = x_hat.cpu()\n",
    "plt.imshow(x_hat.view(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, there is no control over what the outputs are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sampling from the derived distribution\n",
    "\n",
    "Here, we will be sampling from the individual distribution we derived above. Let's assume we would like to generate digit 7. For this, we will need the pdf, $p(z|x=7)$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit to be generated\n",
    "digit = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_x_mu = h_mu[h_labels==digit,:]\n",
    "sns.distplot(pdf_x_mu)\n",
    "mu_x = torch.mean(pdf_x_mu)\n",
    "sigma_x = torch.std(pdf_x_mu)\n",
    "plt.grid(True)\n",
    "plt.title(r'Distribution of $p(z|x={0})$,  ($\\mu=${1:0.4f}, $\\sigma=${2:.4f})'.format(digit, mu_x, sigma_x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_x = torch.distributions.Normal(torch.tensor([mu_x]), torch.tensor([sigma_x])).sample((1,Z_DIM))\n",
    "z_x = z_x.view(1,Z_DIM).to(device)\n",
    "with torch.no_grad():\n",
    "    x_hat = model.decode(z_x)\n",
    "x_hat = x_hat.cpu()\n",
    "plt.imshow(x_hat.view(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is no control over what is being generated. Surely, we made some simplifications here, but they do not contribute to this 'lack of control' over the generative process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sampling from the derived distribution (Sanity Check)\n",
    "Finally, we can use the hidden representation of a particular digit. This is nothig more than a look-up table, and boring (and useless). But let's do this for a sanity check. Assume we would like to generate a digit '5'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit to be generated\n",
    "digit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_d_all = h_mu[h_labels==digit,:]\n",
    "logvar_d_all = h_logvar[h_labels==digit, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(mu_d_all)\n",
    "plt.title(r'$p(z|x=${0}$)$'.format(digit));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random sample (similar to finding a point )\n",
    "n = torch.randint(0,  mu_d_all.size()[0], (1,))\n",
    "mu_d = mu_d_all[n]\n",
    "logvar_d = logvar_d_all[n]\n",
    "z_d = model.reparameterize(mu_d, logvar_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_d = z_d.to(device)\n",
    "with torch.no_grad():\n",
    "    x_hat_d = model.decode(z_d)\n",
    "x_hat_d = x_hat_d.cpu()\n",
    "plt.imshow(x_hat_d.view(28,28), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the decoder works, but the VAE's lack of control over the generative process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacksquare$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
