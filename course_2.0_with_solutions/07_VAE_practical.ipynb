{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Practical: Inelastic Neutron Scattering\n",
    "\n",
    "In [04_CNN_practical.ipynb](04_CNN_practical.ipynb), we have used the inelastic neutron scattering (INS) dataset to practice classification with a convolutional neural network (CNN). In this notebook, we try to make a disentangled variational autoencoder ($\\beta$-VAE) to generate new images from the INS dataset, using CNNs for encoding and decoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# check version\n",
    "print('Using TensorFlow v%s' % tf.__version__)\n",
    "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n",
    "\n",
    "# helpers\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Storage Boilerplate\n",
    "\n",
    "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. To access the data, you need to:\n",
    "\n",
    "1. Run the first cell;\n",
    "2. Follow the link when prompted (you may be asked to log in with your Google account);\n",
    "3. Copy the Google SDK token back into the prompt and press `Enter`;\n",
    "4. Run the second cell and wait until the data folder appears.\n",
    "\n",
    "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will take no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables passed to bash; do not change\n",
    "project_id = 'sciml-workshop'\n",
    "bucket_name = 'sciml-workshop'\n",
    "colab_data_path = '/content/sciml-workshop-data/'\n",
    "\n",
    "try:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    google_colab_env = 'true'\n",
    "    data_path = colab_data_path\n",
    "except:\n",
    "    google_colab_env = 'false'\n",
    "    ###################################################\n",
    "    ######## specify your local data path here ########\n",
    "    ###################################################\n",
    "    data_path = './sciml-workshop-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n",
    "\n",
    "# running locally\n",
    "if ! $1; then\n",
    "    echo \"Running notebook locally.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# already mounted\n",
    "if [ -d $2 ]; then\n",
    "    echo \"Data already mounted.\"\n",
    "    exit\n",
    "fi\n",
    "\n",
    "# mount the bucket\n",
    "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "apt -qq update\n",
    "apt -qq install gcsfuse\n",
    "gcloud config set project $3\n",
    "mkdir $2\n",
    "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dataset\n",
    "\n",
    "The training data are saved in `ins/train.h5`, which contains 20,000 INS images and their one-hot encoded labels identifying an image as either being of the *Dimer* or *Goodenough* Hamiltonian. Similar to [04_CNN_practical.ipynb](04_CNN_practical.ipynb), we first open the dataset using the `tf.data.Dataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image size\n",
    "IMG_HEIGHT = 20\n",
    "IMG_WIDTH = 200\n",
    "N_CHANNELS = 1\n",
    "N_CLASSES = 2\n",
    "\n",
    "# generator\n",
    "def hdf5_generator(path, buffer_size=32):\n",
    "    \"\"\" Load data INS data from disk\n",
    "    \n",
    "    Args:\n",
    "        path: path of the HDF5 file on disk\n",
    "        buffer_size: number of images to read from disk\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as handle:\n",
    "        n_samples, h, w, c = handle['images'].shape\n",
    "        for i in range(0, n_samples, buffer_size):\n",
    "            images = handle['images'][i:i+buffer_size, ..., :1]\n",
    "            labels = handle['labels'][i:i+buffer_size]\n",
    "            yield images, labels\n",
    "\n",
    "# training data\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: hdf5_generator(path=data_path + 'ins-data/train.h5'), \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n",
    "                                                              (None, N_CLASSES,)))\n",
    "# print\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a VAE, we need the images for both input and output. Therefore, we use the `map()` method to change the data from `(X, Y)` to `(X, X)`, where `X` represents the images and `Y` the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dataset = train_dataset.map(lambda X, Y: (X, X))\n",
    "print(train_image_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will load the first buffer (with 32 data by default) to memory and plot some images and labels from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the first buffer (with 32 data by default)\n",
    "images, labels = list(train_dataset.take(1))[0]\n",
    "\n",
    "# plot some images and labels from it\n",
    "nplot = 10\n",
    "fig, axes = plt.subplots(nplot // 2, 2, figsize=(16, nplot / 1.5), dpi=100)\n",
    "for ax, image, label in zip(axes.flatten(), images, labels):\n",
    "    ax.matshow(np.squeeze(image))\n",
    "    ax.set_xlabel('0: Dimer' if label[0] < .5 else '1: Goodenough', c='k')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\beta$-VAE for  Image Generation\n",
    "\n",
    "This $\\beta$-VAE will be a combination of the CNN architecture in [04_CNN_practical.ipynb](04_CNN_practical.ipynb) and the $\\beta$-VAE implementation in [06_VAE_basics.ipynb](06_VAE_basics.ipynb).\n",
    "\n",
    "\n",
    "## 1. Encoder and decoder\n",
    "\n",
    "First, we need to specify the latent dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent dimension\n",
    "latent_dim = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extend the CNN in [04_CNN_practical.ipynb](04_CNN_practical.ipynb) to an encoder and a decoder. \n",
    "\n",
    "**Suggested Answer for Encoder** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# sampling z with (z_mean, z_log_var)\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "# build the encoder\n",
    "image_input = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS))\n",
    "x = layers.Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same')(image_input)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z_output = Sampling()([z_mean, z_log_var])\n",
    "encoder_BVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n",
    "encoder_BVAE.summary()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "**Suggested Answer for Decoder** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# build the decoder\n",
    "z_input = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(800, activation=\"relu\")(z_input)\n",
    "x = layers.Reshape((2, 25, 16))(x)\n",
    "x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "x = layers.ZeroPadding2D((1, 0))(x)\n",
    "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "x = layers.Conv2DTranspose(8, kernel_size=(5, 5), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "image_output = layers.Conv2DTranspose(1, kernel_size=(3, 3), activation='linear', padding='same')(x)\n",
    "decoder_BVAE = keras.Model(z_input, image_output)\n",
    "decoder_BVAE.summary()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling z with (z_mean, z_log_var)\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "# build the encoder\n",
    "image_input = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS))\n",
    "x = layers.Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same')(image_input)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Flatten()(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z_output = Sampling()([z_mean, z_log_var])\n",
    "encoder_BVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n",
    "encoder_BVAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the decoder\n",
    "z_input = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(800, activation=\"relu\")(z_input)\n",
    "x = layers.Reshape((2, 25, 16))(x)\n",
    "x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "x = layers.ZeroPadding2D((1, 0))(x)\n",
    "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D(size=(2, 2))(x)\n",
    "x = layers.Conv2DTranspose(8, kernel_size=(5, 5), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "image_output = layers.Conv2DTranspose(1, kernel_size=(3, 3), activation='linear', padding='same')(x)\n",
    "decoder_BVAE = keras.Model(z_input, image_output)\n",
    "decoder_BVAE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The `BVAE` class\n",
    "\n",
    "The `BVAE` class will be exactly the same as implemented in [06_VAE_basics.ipynb](06_VAE_basics.ipynb).\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# BVAE class\n",
    "class BVAE(keras.Model):\n",
    "    # constructor\n",
    "    ########################################################\n",
    "    ######## NEW: passing beta as an extra argument ########\n",
    "    ########################################################\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(BVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "\n",
    "    # customise train_step() to implement the loss \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # encoding\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            # decoding\n",
    "            x_prime = self.decoder(z)\n",
    "            # reconstruction error by binary crossentropy loss\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime))\n",
    "            reconstruction_loss *= IMG_HEIGHT * IMG_WIDTH\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # loss = reconstruction error + KL divergence\n",
    "            #######################################\n",
    "            ######## NEW: scale KL by beta ########\n",
    "            #######################################\n",
    "            loss = reconstruction_loss + self.beta * kl_loss\n",
    "        # apply gradient\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        #######################################\n",
    "        ######### NEW: log scaled KL ##########\n",
    "        #######################################\n",
    "        # return loss for metrics log\n",
    "        return {\"loss\": loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"beta_kl_loss\": self.beta * kl_loss}\n",
    "\n",
    "# build the BVAE\n",
    "########################################\n",
    "######## NEW: pass beta to BVAE ########\n",
    "########################################\n",
    "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=5.)\n",
    "\n",
    "# compile the BVAE\n",
    "bvae_model.compile(optimizer=keras.optimizers.Adam())\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BVAE class\n",
    "class BVAE(keras.Model):\n",
    "    # constructor\n",
    "    ########################################################\n",
    "    ######## NEW: passing beta as an extra argument ########\n",
    "    ########################################################\n",
    "    def __init__(self, encoder, decoder, beta, **kwargs):\n",
    "        super(BVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "\n",
    "    # customise train_step() to implement the loss \n",
    "    def train_step(self, x):\n",
    "        if isinstance(x, tuple):\n",
    "            x = x[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            # encoding\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            # decoding\n",
    "            x_prime = self.decoder(z)\n",
    "            # reconstruction error by binary crossentropy loss\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime))\n",
    "            reconstruction_loss *= IMG_HEIGHT * IMG_WIDTH\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            # loss = reconstruction error + KL divergence\n",
    "            #######################################\n",
    "            ######## NEW: scale KL by beta ########\n",
    "            #######################################\n",
    "            loss = reconstruction_loss + self.beta * kl_loss\n",
    "        # apply gradient\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        #######################################\n",
    "        ######### NEW: log scaled KL ##########\n",
    "        #######################################\n",
    "        # return loss for metrics log\n",
    "        return {\"loss\": loss,\n",
    "                \"reconstruction_loss\": reconstruction_loss,\n",
    "                \"beta_kl_loss\": self.beta * kl_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build and train the `BVAE` model\n",
    "\n",
    "Now, build the `BVAE` model and train it with the INS dataset. Let us first use $\\beta=5$ and start with 5 epochs.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# build the BVAE\n",
    "########################################\n",
    "######## NEW: pass beta to BVAE ########\n",
    "########################################\n",
    "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=5.)\n",
    "\n",
    "# compile the BVAE\n",
    "bvae_model.compile(optimizer=keras.optimizers.Adam())\n",
    "    \n",
    "# train the BVAE\n",
    "training_history_BAVE = bvae_model.fit(train_image_dataset, epochs=5, batch_size=32)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the BVAE\n",
    "########################################\n",
    "######## NEW: pass beta to BVAE ########\n",
    "########################################\n",
    "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=5.)\n",
    "\n",
    "# compile the BVAE\n",
    "bvae_model.compile(optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# train the BVAE\n",
    "training_history_BAVE = bvae_model.fit(train_image_dataset, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate images\n",
    "\n",
    "Finally, we can generate new images using the decoder.\n",
    "\n",
    "**Suggested Answer** \n",
    "\n",
    "<details> <summary>Show / Hide</summary> \n",
    "<p>\n",
    "    \n",
    "```python\n",
    "# generate images from the latent space\n",
    "def generate_images_latent(decoder, n_generation, feature_range):\n",
    "    # randomly sample the latent space\n",
    "    latent = []\n",
    "    for dim in range(latent_dim):\n",
    "        if len(np.array(feature_range).shape) == 1:\n",
    "            # only one range provided; used it for all dimensions\n",
    "            latent.append(np.random.uniform(feature_range[0], feature_range[1], \n",
    "                                            n_generation))\n",
    "        else:\n",
    "            # range provided for each dimension\n",
    "            latent.append(np.random.uniform(feature_range[dim][0], feature_range[dim][1], \n",
    "                                            n_generation))\n",
    "    latent = np.array(latent).T\n",
    "    \n",
    "    # decode images\n",
    "    decodings = decoder.predict(latent)\n",
    "    \n",
    "    # plot generated images\n",
    "    fig, axes = plt.subplots(n_generation // 2, 2, figsize=(16, n_generation / 2), dpi=100)\n",
    "    for ax, image in zip(axes.flatten(), decodings):\n",
    "        ax.matshow(image)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()  \n",
    "\n",
    "# generate random images sampled between [-1, 1]\n",
    "generate_images_latent(decoder_BVAE, n_generation=30, feature_range=[-1, 1])\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images from the latent space\n",
    "def generate_images_latent(decoder, n_generation, feature_range):\n",
    "    # randomly sample the latent space\n",
    "    latent = []\n",
    "    for dim in range(latent_dim):\n",
    "        if len(np.array(feature_range).shape) == 1:\n",
    "            # only one range provided; used it for all dimensions\n",
    "            latent.append(np.random.uniform(feature_range[0], feature_range[1], \n",
    "                                            n_generation))\n",
    "        else:\n",
    "            # range provided for each dimension\n",
    "            latent.append(np.random.uniform(feature_range[dim][0], feature_range[dim][1], \n",
    "                                            n_generation))\n",
    "    latent = np.array(latent).T\n",
    "    \n",
    "    # decode images\n",
    "    decodings = decoder.predict(latent)\n",
    "    \n",
    "    # plot generated images\n",
    "    fig, axes = plt.subplots(n_generation // 2, 2, figsize=(16, n_generation / 2), dpi=100)\n",
    "    for ax, image in zip(axes.flatten(), decodings):\n",
    "        ax.matshow(np.squeeze(image))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()  \n",
    "\n",
    "# generate random images sampled between [-1, 1]\n",
    "generate_images_latent(decoder_BVAE, n_generation=30, feature_range=[-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises:\n",
    "\n",
    "1. Tune `latent_dim` and `beta` (and use more epochs) to improve the quality of image generation.\n",
    "2. Implement a conditional VAE for this INS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
